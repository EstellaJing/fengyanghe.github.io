---
layout: post
title: "world models"
description: "David Ha , J¨urgen Schmidhuber. Reading by LiangXingXing"
category: "study" 
tags: []
image: assets/images/worldmodel/worldmodel1.jpg
---
直觉认为人类通过其有限感知构建世界的物理模型，并根据此模型进行决策行动。证据表明，给定时刻下，我们的感知是我们大脑根据物理模型对未来做出的预测。该模型不仅可以预测未来，而且主要是根据当前的行为去预测可能发生的未来，帮助我们快速反应和躲避危险。  
David Ha , J¨urgen Schmidhuber.   
Reading  by Xingxing Liang  <!--excerpt-->

### 问题描述
直觉认为人类通过其有限感知构建世界的物理模型，并根据此模型进行决策行动。证据表明，给定时刻下，我们的感知是我们大脑根据物理模型对未来做出的预测。该模型不仅可以预测未来，而且主要是根据当前的行为去预测可能发生的未来，帮助我们快速反应和躲避危险。
好的强化学习模型受益于对当前和过去状态的良好表征，以及对未来的预测模型。然而，对于传统RL来说，难以学习参数庞大的模型，进而使得现有RL模型参数较少。较少的参数可以使得迭代速度加快。
而本文的作者希望训练出庞大的基于RNN的模型，用于使得RL能够对时空数据具有很好的表征。因而，作者将庞大的模型从传统的RL中剥离出来，划分为两个模型，包含大量参数的world model和包含少量参数的controller model。文中首先采用无监督的训练模式训练出world model，之后利用该模型去训练controller model。

我们尝试构建RL的环境生成神经网络模型。我们的世界模型可以以无监督的方式进行快速训练，学习环境的压缩空间和时间表示。通过使用从该世界模型中提取作为agent输入的特征，我们可以训练一个非常紧凑和简单的策略，以解决所需的任务。我们甚至可以完全在自己的世界模型所产生的梦幻梦境中训练我们的agent，并将此agent转移回实际环境中。

### 解决的途径
#### 基本过程
1.	模型结构示意图  
![Model Structure](/assets/images/worldmodel/worldmodel1.jpg "Model Structure")  
输入：未经处理的图片信息  
输出：决策动作  
过程：a 将未经处理的图片信息输入至world model的基于VAE自动编码的V模型中，获得对当前状态的表征向量z<sub>t</sub>;  
      B将当前状态的z<sub>t</sub>和RNN隐藏信息的h<sub>t</sub>输入至controller model中，获得动作输出。  
      C将当前状态的向量表示z<sub>t</sub>、a<sub>t</sub>和RNN模型的上一隐藏层信息h<sub>t</sub>输入至world model中的M模型中，获得隐藏层信息h<sub>t+1</sub>以及预测的下一时刻状态表征z<sub>t+1</sub>。  
	    
2.	VAE（V）模型  
    ![VAE](/assets/images/worldmodel/worldmodel2.jpg "VAE2")  
    V模型的目的是学习观测输入的抽象、压缩表征。  
  
    神经网络架构图：  
    ![NN Structure](/assets/images/worldmodel/worldmodel3.jpg "NN Structure")  
    Stride=2.使用输入与输出的L^2距离+KL散度作为损失函数。   
    ![NN Structure](/assets/images/worldmodel/worldmodel4.jpg "NN Structure")  
  
3.	MDN-RNN（M）模型  
![NN Structure](/assets/images/worldmodel/worldmodel5.jpg "NN Structure")  
M模型的目的是输出对未来的预测，采用混合密度网络输出对未来表征的概率分布，即P(z<sub>t+1</sub>|a<sub>t</sub>,z<sub>t</sub>,h<sub>t</sub>)。τ是用来控制模型的不确定性。  
![NN Structure](/assets/images/worldmodel/worldmodel6.jpg "NN Structure")  
在A Neural Representation of Sketch Drawings论文中这么推导这个结构（输出为一个二元）。  
RNN输出的结果包含M个高斯分布，每个高斯分布所需要的参数为![NN Structure](/assets/images/worldmodel/worldmodel20.jpg "NN Structure")。每个高斯分布的选择概率为softmax分布。作者使用exp函数确保标准差值非负和tanh函数确保相关系数取值（-1,1）之间。之后通过采样，获得相邻两状态的差值的概率。为了增加随机性，作者做了如下操作![NN Structure](/assets/images/worldmodel/worldmodel21.jpg "NN Structure")。原文作者限定了τ∈[0,1]。训练时的损失函数包含两部分，偏移量损失和KL散度：  
![NN Structure](/assets/images/worldmodel/worldmodel7.jpg "NN Structure")   
![NN Structure](/assets/images/worldmodel/worldmodel8.jpg "NN Structure")   
![NN Structure](/assets/images/worldmodel/worldmodel9.jpg "NN Structure")   
在本文的操作中，作者使用了5个高斯分布，并且没有计算协方差，只是计算了方差。同样作者令τ∈[0,+∞]，进而增加环境的随机性。  
  
4.	Controller（C）model  
                                         a<sub>t</sub>=W<sub>c</sub>[z<sub>t</sub>,h<sub>t</sub>]+b<sub>c</sub>  
	C模型是线性模型，庞大的复杂参数被留在world model中。C模型参数较少，因而可以进行快速迭代。  
	参数设计：利用tanh激活函数去限定决策变量的取值。  
We would give the C Model a feature vector as its input, consisting of z and the hidden state of the MDN-RNN. In the Car Racing task, this hidden state is the output vector h of the LSTM, while for the Doom task it is both the cell vector c and the output vector h of the LSTM。  
训练方法是利用CMA-ES（https://github.com/CMA-ES/pycma）。并行64核，每个核带不同的参数，每个核仿真16次，作为该核的累积奖赏评价。每运行25代，将效果好的agent拿出来测试64*16次，计算其平均奖赏。  

5.	混合V,M和C  
![NN Structure](/assets/images/worldmodel/worldmodel10.jpg "NN Structure")  
首先，未经处理的观测由V模型处理成 。  
其次，将z<sub>t</sub>和M模型的隐藏层信息 一并传给C模型，做出决策，对环境产生影响。  
之后，将z<sub>t</sub>，a<sub>t</sub>，h<sub>t</sub>传给M模型，生成隐藏层信息 以及下一时刻预测信息z<sub>t+1</sub>。  
伪码表示：  
![NN Structure](/assets/images/worldmodel/worldmodel11.jpg "NN Structure")  
  
6.	迭代训练过程：  
对于简单的任务，随机采样的样本也可以构建出全部的环境，但是，如果环境很复杂，随机样本只能给出部分环境模型，因而需要迭代的对这些样本进行训练，过程如下：  
![NN Structure](/assets/images/worldmodel/worldmodel12.jpg "NN Structure")   
简单的任务迭代一次就可以了。
 we can adapt and reuse M’s training loss function to encourage curiosity.   
   
### 算法流程图：
无

### 评估指标：
百次实验取平均  
  
### 实验数据或案例：  
1.	Car racing 实验（CarRacing-V0）  
（1）游戏介绍：Easiest continuous control task to learn from pixels, a top-down racing environment. Discreet control is reasonable in this environment as well, on/off discretisation is fine. State consists of 96x96 pixels. Reward is -0.1 every frame and +1000/N for every track tile visited, where N is the total number of tiles in track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points. Episode finishes when all tiles are visited. Some indicators shown at the bottom of the window and the state RGB buffer. From left to right: true speed, four ABS sensors, steering wheel position, gyroscope. Control three continuous actions：steering left/right, acceleration, and brake.  
![NN Structure](/assets/images/worldmodel/worldmodel13.jpg "NN Structure")  
（2）实验过程设计：  
![NN Structure](/assets/images/worldmodel/worldmodel14.jpg "NN Structure")   
（3）实验结果：  
 ![NN Structure](/assets/images/worldmodel/worldmodel15.jpg "NN Structure")   
 ![NN Structure](/assets/images/worldmodel/worldmodel16.jpg "NN Structure")   
（4）梦境研究:  
M可以根据当前状态、动作以及隐藏信息预测未来状态，因而可以用来构建虚拟环境，直接对C模型进行训练。不再依靠V模型，V模型在此处的作用是重构2D环境，看效果的。  
  
2.	VizDoom 实验  
实验目的：测试梦境中学习的策略，能否成功应用至真实环境中。  
（1）游戏介绍：http://vizdoom.cs.put.edu.pl  
本文测试环境是VizDoom：take cover  
The agent must learn to avoid fireballs shot by monsters from the other side of the room with the sole intent of killing the agent. There are no explicit rewards in this environment, so to mimic natural selection, the cumulative reward can be defined to be the number of time steps the agent manages to stay alive in a rollout. Each rollout in the environment runs for a maximum of 2100 frames (~ 60 seconds), and the task is considered solved when the average survival time over 100 consecutive rollouts is greater than 750 frames。（决策是离散变量，左、不动、右。用连续变量去处理。）  
![NN Structure](/assets/images/worldmodel/worldmodel17.jpg "NN Structure")   
（2）实验过程设计：  
![NN Structure](/assets/images/worldmodel/worldmodel18.jpg "NN Structure")   
同Car racing相比，这个游戏需要预测下一时刻agent是否死亡，因而还需要预测done信息。因而，该M模型构建了一个完全的RL环境。  
（3）梦境训练  
形成了完整的RL环境，M相当于环境模型。在训练时，加入状态不确定性，增强agent的能力。  
（4）策略迁移  
没做处理，直接拿来用，效果很棒，存活在1100帧附近（100次试验）  
（5）欺骗world model  
我们以前玩游戏总是喜欢找游戏的bug，取巧的假装玩的很好，实际这样就让我们丧失了成为顶级玩家的机会，是不对的。    
由于我们的world model只是真实环境的概率模型，产生的轨迹可能不太符合真实环境的相关规律。C模型可能会利用M模型中的一些bug获胜。     
基于此，之前的研究也不是完全用学习到的模型去替换真实环境。我们主要是利用调节梦境的动态性来克服欺骗world model的产生。下图，并没有利用真实的环境去训练，而是利用可调节DoomRNN去训练C模型，真实环境只是为了去获取样本数据用来训练DoomRNN。  
![NN Structure](/assets/images/worldmodel/worldmodel19.jpg "NN Structure")   

### 应用方向  
  
### 研究领域现状  
动态性研究：  
最近的PILCO：收集数据，利用高斯过程去学习系统的动态性，之后用该模型去生成数据训练决策模型。然而高斯模型针对高维观测计算复杂度很高，对低维观测有效。还有一些人利用贝叶斯网络去替换GP学习环境的动态性，但都是低维的输入。  
  
处理基于图片的观测任务：  
也有人做，思路和作者一致，尤其是Finn, Chelsea. Model-based rl lecture at deep rl bootcamp  2017, 2017.    
用CNN处理图片的近况，许多方法使用FNNs去预测下一帧图片。Hallucination with RNNs一文展示了利用RNN去学习atari游戏环境的强大能力。  
  
RL方法选择：  
我们使用ES。有些人已经证明这方法不错。并且在DRL之前，大家热衷的就是进化方法。  

### 剩余问题
VAE模型可能会把任务无关的观测进行编码。无监督的学习方式可能会丢掉一些对任务有用的信息，而保留无关的信息。如果将V模型和能够预测奖赏的M模型结合，可能效果会好一些。另外，从神经科学的角度出发，抽取更好的状态表征方式。  
	World model的容量有限。Lstm不能有效记住这些数据信息。但是我们的人脑就会记住，是不是很神奇。  
为C模型加上循环，或者C-M模型放一起。  
  
### 源代码
暂无  








  




    



  


